---
title: "Homework 3 - Math 605, Spring 2022"
author: "(replace by Student's Name)"
date: "Due date = 04/20/2022"
output:
  pdf_document:
    includes:
      in_header: header.tex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


**You may discuss homework problems with other students, but you have to prepare the written assignments yourself.**

**Please knit all your answers, the computer code and the figures into one PDF file, and submit a copy to Gradescope.**

**This is the third assignment. You must complete the Rmd file and I will not accept a hand-written copy.**
install.packages("tinytex")
library("tinytex")
tinytex::install_tinytex()

## Problem 1: 

For each of the 

(1) hinge loss: $\phi(z) = (1-z)_+$,
Ans:
 By direct consideration of the piecewise-linear form of $C_{\eta}(z)$, it is easy to see that for 
$\eta=0$,for each $z\leq-1$ makes $C_{\eta}(z)$ vanish. The same holds for z$\geq 1$when $\eta=1.$ Now for $\eta\in(0,1)$, we see that $C_\eta$ decreases strictly on$(-\infty,-1]$ and increases strictly on $[1,\infty)$. Thus, any minima must lie in [-1,1]. But $C_\eta$ is linear on [-1,1], sp minimum must be attained at 1 for $\eta>1/2$,-1 for $\eta<1/2$ and anywhere in [-1,1] for $\eta=1/2.$ We have obtained the stationary point by solving it: $z^*(\eta)=sign(\eta-1/2)$ for all $\eta\in(-1,1)$ other that 1/2. From $z^*(\eta)=sign(\eta-1/2)$, yields minima at 0,1/2 and 1, so we can choose to extend it to whole unit interval. Thus, we obtain $H(\eta)=2min\{\eta,1-\eta\}$, For $0\leq\eta\leq 1$. Since $H^-((1+\theta)/2)\equiv \phi(0)=1$, we have $\psi^*(\theta)=\theta$ and $\psi=\psi^*$ by convexity. Also it is easy to see that $\eta\in\{0,1\}$, this inequality also holds. Then $H^-(\eta)>H(\eta)$Thus, hinge loss is classification calibrated.
$\mid\eta-1/2\mid=1/2(1-H_{\phi}(\eta))'$ This implies $c=1/2, r=1$








(2) logistic loss: $\phi(z) = \log(1+\exp(z))$, and
\
Ans,\
Taking the derivative of the following function and set it equal to zero: \
$f(z)=\eta log(1+exp(z))+(1-\eta)log(1+exp(-z))$ .THen we obtain $z=log\frac{1-\eta}{\eta}$, since this is convex, we have a minimum. \
$H_\phi(\eta)=inf_\alpha f(z)=\eta log\frac{1}{\eta}+(1-\eta)log\frac{1}{1-\eta}$.
Next, $H_\phi(\eta)^-=f(0)=log2$. Hence, $H_\phi(\eta)<H_\phi(\eta)^-$, when $\eta\neq 1/2$.
So, logistic loss is classification calibrated. 
$\psi(\theta)=\phi(0)-H((1+\theta)/2)=(1+\theta)/2*log(1+\theta)+(1-\theta)/2*log(1-\theta)$.




(3) exponential loss: $\phi(z) = \exp(z)$,
 exponential loss. For $\eta\in(0,1)$, solve for the stationary pont yields the minimizer $z*(\eta)=log(\frac{1-\eta}{\eta})/2$. plug in the z*, we have$H(\eta)=2\sqrt{\eta(1-\eta)}$. Also,  this expression is true for $\eta$ equal to 0 or 1. Then, $H^-((1+\theta)/2)\equiv exp(0)=1$, thus. $\psi'(\theta)=1-\sqrt{1-\theta^2}$. Sincere $\psi'$ is convex, $\psi=psi'$.\
 Finally, for $0<\eta<1, sign(z^*(\eta))=sign(\eta-1/2).$Also, a sequence($z_i$) can achieve H at $\eta$=0, if it diverge to $-\infty$;$\eta=1$,if it diverges to $\infty.$
Thus it follows exponential loss is classification calibrated. 






\newpage

## Problem 2:

(a) Consider the constant function, $k(x,y) = c$ for all $x$, $y$. Is $k$ a symmetric positive semi-definite kernel? If not, explain why not. If so, describe its reproducing kernel Hilbert space.

Ans: No, If c is negative number and then $x'cx\leq 0.$ On the other hand if c is positive, then it is a symmetric positive semi-definite kernel. 

(b) For two symmetric, positive semi-definite kernels $k_1$, $k_2$ defined on the same space, let $k$ be their minimum, $k(u,v) = \min\{k_1(u,v),k_2(u,v)\}$. Is $k$ also a symmetric positive semi-definite kernel? If not, explain why not. If so, describe its reproducing kernel Hilbert space.
Ans: Since $k(u,v) = \min\{k_1(u,v),k_2(u,v)\}=\min\{k_1(v,u),k_2(v,u)\}=k(v,u)$, k is symmetry. Next we want to show k is positive semi-definite. Let x be any value in the domain, 
then,
$$
x'*k(u,v) *x=x'*\min\{k_1(v,u),k_2(v,u)\} *x=\min(x'*\{k_1(v,u),k_2(v,u)\} *x)
=\min(x'*\{k_1(v,u)\} *x,x'*\{k_1(v,u)\} *x)
$$
Since  $x'*\{k_1(v,u)\} *x \geq 0, x'*\{k_1(v,u)\} *x\geq 0$, thus their minimum is greater or equal to zero, as required. 

(c) Consider a symmetric, positive semi-definite kernel, $k$ defined on $\mathcal{X}$, and let $K$ denote the kernel matrix corresponding to the set $\{x_1,\dots,x_n\}\in\mathcal{X}$ (that is, $K_{ij}=k(x_i,x_j)$). Suppose that $K$ has full rank. For $u\in\mathcal{X}$, define a vector $k_x(u) = (k(u,x_1),\dots,k(u,x_n))^T$, and for $u,v \in \mathcal{X}$, define 
$$\tilde k(u,v) = k(u,v) - k_x(u)^T K^{-1}k_x(u)$$
Show that $\tilde k$ is also a symmetric, positive semi-definite kernel.
(Hint: You might use the observation that any kernel matrix is the covariance matrix of some
Gaussian random vector, and consider conditional covariances. Check notes with your friends who are taking Math 570.)
\
Ans:\ By the theorem: The conditional distribution of MVN is MVN on lecture 3 page 33. Now we can use the hint that any kernel matrix is the covariance matrix of some
Gaussian random vector, let $k(u,v)=\Sigma_{11}$, $k_x(u)=\Sigma{21}$ and $K=\Sigma_{22}$. Then $\tilde k(u,v)$ represent the covariance matirx of the conditional normal distribution, thus, it is symmetry, positive semidefinite. 


(d) Let the input space be $\mathcal{X} = \mathbb{R}^d$. Define the kernel to be 1 for closeby points and 0 otherwise:
$$k(x,x') = \begin{cases}
1,&\text{if }\|x-x'\|_2\le 1\\
0,& \text{otherwise.}
\end{cases}$$
Is k a valid kernel? Justify your result either way.\
ANS:\
NO. To see this, we will show by a counter example. For this kernel to be positive definite, the following inequality must be verified for every positive $n\in\mathbb{N}$, and $x_1,...,x_n\in \mathbb{R}$, also $a_1,...,a_n\in \mathbb{R}$, such that $\sum_{i,j}a_ia_jK(x_i,x_j)\geq0$. Let n=3, $a_1-a_2=1 ,a_3=-2, x_1=2,x_2=0,x_3=1$, then, \
$\sum_{i,j}a_ia_jK(x_i,x_j)=a_1^2+a_2^2+a_3^2+2a_2a_3+2a_1a_3=-2$\


\newpage

(e) Suppose $k'$ is a valid kernel. Define the normalized kernel $k$ based on $k'$ as follows:
$$k(x,x') = \frac{k'(x,x')}{\sqrt{k'(x,x)k'(x',x')}}.$$
Is $k$ a valid kernel? Justify your result either way.\
Ans: 
Yes , this is valid kernel. To see this, define $\psi'(x)=(\psi'_1(x),...,\psi'_N(x))$ be the feature map for $K'$. Define $\psi(x)$ as $\psi_i(x)=\frac{\psi_i'(x)}{||\psi'(x)||}$. Then $k()$ calculates the inner product
for $\psi()$.


(f) Suppose $k': \mathcal{Y}\times\mathcal{Y}\mapsto \mathbb{R}$ is a valid kernel, and let $\mathcal{F}$ be a finite collection of functions from $\mathcal{X}$ to $\mathcal{Y}$. Define $k$ as follows:
$$k(x,x') =\sum_{f\in\mathcal{F}}\sum_{f'\in\mathcal{F}}
k'(f(x),f'(x')).$$
Is $k$ a valid kernel? Justify your result either way.\

Ans: Yes , $k$ is a valid kernel. TO see this, since $k'$ is a valid kernel, then for any i,j, $k'(f_i(x),f'_j(x'))=k(y,y')$ for some $y,y'\in \mathcal{Y}$. Since $\mathcal{F}$ be a finite collection of functions, $$k(x,x') =\sum_{f\in\mathcal{F}}\sum_{f'\in\mathcal{F}}
k'(f(x),f'(x')).$$ is finite sum of valid kernel. Thus, it is valid kernel. \
It suffices to show the sum of two valid kernels is a valid kernel. To see this, 
Let $\psi'(x)=(\psi'_1(x),...,\psi'_N(x))$ and $\psi^2(x)=(\psi^2_1(x),...,\psi^2_{N_2}(x))$ be  the feature map for two valid kernel K1 and K2. Define $\psi(x)=(\psi'_1(x),...,\psi'_N(x),\psi^2_1(x),...,\psi^2_{N_2}(x))$ then

$\psi(x)\psi(y)=\psi'(x)\psi'(y)+\psi^2(x)\psi^2(y)$ as required.


(g) Let $\mathcal{X} = [0,1].$ Define: $k(x,x') = \min(x,x').$ Show that $k$ is a valid kernel. Hint: given $n$ points $x_1 ,\dots,x_n$, try to construct a $n$-dimensional multivariate Gaussian distribution with covariance matrix equal to the kernel matrix.
\
Ans:\
First we want to show k is positive semidefinite: 
$\forall\alpha\in\mathcal{R}^n,\forall(x_1,...,x_n)\in [0,1]^n$ \
we have
$\sum_{i,j}\alpha_i\alpha_j min(x_i,x_j)=\sum_{i,j}\alpha_i\alpha_j\int_0^1  1_{t\leq x}(t 1_{t\leq y}(t)dt$\
$=\int^1_0(\sum_{i=1}^n\alpha_i 1_{t\leq x_i})(\sum_{i=1}^n\alpha_j1_{t\leq x_j})=\int^1_0(\sum_{i=1}^n\alpha_i 1_{t\leq x_i})^2 \geq0$

(h) It turns out that $k$ in (g) above is the reproducing kernel for an RKHS over the space of functions differentiable almost everywhere:
$$\mathcal{H} = \{f: f(0) = 0\text{ and }f' \in L^2([0,1])\},$$
with inner product defined as $\langle f,g\rangle = \int_0^1 f'(x)g'(x)dx$. Verify that the reproducing property for $k$ holds (that $\langle k(Â·,x),f\rangle = f(x)$).

Ans:\
Suppose $f\in\mathcal{H},x\in[0,1]$\
$<f,k_x>=\int_0^1f'(t)1_{t\leq x}dt=\int_0^xf'(t)dt=f(x)-f(0)=f(x)$\
Thus, it is RKHS.


(i) Given a set $\Omega$, let $\mathcal{X} = 2^\Omega$, that is, the set of all subsets of $\Omega$. Define a function $k(A,B) = 2^{|A\cap B|}$ where $A,B\in\mathcal{X}$ and $|\cdot|$ is the cardinality. Show that $k$ is a positive semi-definite kernel function.
\
ANS: 
\
I will do this by two ways:
first way: It is clear that $k(A,B)$ is symmetry, to see it is a semi-definite, \
$k(A,B)=2^{\mid A\cap B\mid}\geq 1.$ We know that 1 is a constant function, which is semi-definite. Hence, $k(A,B)$ is also semi-positive def.\
Second way:\
We can try to find the feature map, suppose $\Omega=\{a_1,a_2,...,a_N\}$ for some N. Let $\phi(A)=(x_1,x_2,...,x_N)^T$, where each $x_i\in \{0,1\}$ that if $a_i\in A$ then $x_i=1$, otherwise  if $a_i\notin A$ then $x_i=0$. \
Let $A_1,...A_n\in \mathcal{X}$ and $c_1,...,c_n\in R$\
$\sum_i\sum_jc_i\bar c_jk(A_i,A_j)=\sum_i\sum_jc_i\bar c_j2^{<\phi(A_i),\phi(A_j)>}$\
$\sum_ic_i2^{\phi(A_i)/2},\sum_ic_j2^{\phi(A_j)/2}=||\sum_ic_i2^{\phi(A_i)/2}||^2\geq 0$ as required.

\newpage 
## Problem 3

Download the data set `regression.dat`, which defines a regression problem in $\mathbb{R}^{10}$. (The first 10 columns correspond to ($x_1,\dots,x_{10}$) and the final column corresponds to $y$.)

(a) Fit a linear regression to these data and report the sum of squared errors on the test
set `regression.test`.
```{r}

regression22 <- read.table("C:/Users/xiyon/OneDrive/Desktop/605/hw 3/regression22.test", quote="\"", comment.char="")
regression <- read.table("C:/Users/xiyon/OneDrive/Desktop/605/hw 3/regression.dat", quote="\"", comment.char="")
model <- lm(V11~., data = regression)
summary(model)


predictions <-predict( model,newdata=regression22)

sse <- sum((predictions - regression22$V11)^2)
sse

```
(b) Use ordinary PCA and reduce the dimensionality of the covariate space to two dimensions. Fit a linear regression and report the sum of squared errors on the test set `regression.test`.
```{r}
aa<-prcomp(regression[,1:10],rank=2)


model2 <- lm(regression$V11~., data = data.frame(aa$x))
summary(model2)
regression22new<-scale(regression22[,1:10],aa$center)%*%aa$rotation

predictionss <-predict( model2,newdata=data.frame(regression22new))

sse <- sum((predictionss - regression22$V11)^2)
sse
```

(c) Use kernel PCA with a Gaussian kernel, and reduce the dimensionality of the covariate space to two dimensions. (Propose and implement a method for choosing the bandwidth parameter in the kernal function). Fit a linear regression and report the sum of squared errors on the test set `regression.test`.



```{r}

distance<-function(x1,x2){ 
  x=rowSums(x1^2)
  xy=x1%*%t(x2)
  y=matrix(rowSums(x2^2),dim(x1),dim(x2),byrow=TRUE)
  y+x-2*xy
  

}


Dist=distance(as.matrix(regression[,1:10]),as.matrix(regression[,1:10]))

lowddist=Dist[lower.tri(Dist)]

dim(lowddist)
bandwith=quantile(lowddist,seq(0,0.03,0.005))^.5

bandwith



gaussfunction=function(Dist,sigma){
  exp(-Dist/sigma^2)
  
  
}
library(caret)
set.seed(7)
fold=createFolds(1:60,7)

fold

(crossvalmean=sapply(bandwith,function(sigma){
  dist22=gaussfunction(Dist,sigma)

  
  error=sapply(1:7,function(f){
    num=fold[[f]]
    A<-eigen(dist22[-num,-num])$vectors[,1:2]
    K=dist22[-num,-num]%*%A
    mod33=lm(regression$V11[-num]~K)
    vaK=dist22[num,-num]%*%A
pred=vaK%*%mod33$coef[-1]+mod33$coef[1]    
mean((pred-regression$V11[num])^2)
  })
  
  mean(error)
  
}))



min=which.min(crossvalmean)
dist22=gaussfunction(Dist,bandwith[min])
B=eigen(dist22)$vectors[,1:2]
C=dist22%*%B
mod55=lm(regression$V11~C)

test=distance(as.matrix(regression22[,1:10]),as.matrix(regression[,1:10]))
testK=gaussfunction(test,bandwith[min])%*%B
predtest=testK%*%mod55$coef[-1]+mod55$coef[1]
sum((predtest-regression22$V11)^2)





```